<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Mixing Realities</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/gruvbox-light.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2><em>mixing</em> realities:</h2>
					<img src="images/vr_icon.svg" height="200px">
					<p><small>an <em>affordance-oriented</em> framework for designing virtual spaces</small>
					<br><small>Sean Chua / 1006466</small></p>
				</section>
				<section>
					<h4>
						Why do some <em>virtual experiences</em> feel good and not others?
					</h4>
					<p class="fragment fade-in">
						(and how do we make them good?)
					</p>
				</section>
				<section data-auto-animate>
					<h4>
						Theory of affordances
					</h4>
					<p>
						When you have many options for moving through a space (<em>and the space itself communicates to you</em> the means of movement) you will feel more immersed in the space.
					</p>
					<p>
						Spatial cognition is experienced by the body; an <em>affordance</em> represents a potential avenue of movement through space.
					</p>
					<p style="font-size:0.5em;">
						Carrillo Quiroga, P. and Chacón Hernández, J.C., 2021. The Perception of Space in Virtual Reality, Correlation Between Affordances and Spatial Presence. Entreciencias: diálogos en la sociedad del conocimiento, 9(23).
					</p>
				</section>
				<section data-auto-animate>
					<h4>
						Theory of affordances
					</h4>
					<p>
						Taking this framework, we may explain why shoddily-made VR landscapes don't have mass appeal&mdash;they&rsquo;re <em>just not providing enough</em> affordances.
					</p>
					<p>
						Possible link to <em>Zoom fatigue</em>&mdash;the IRL affordance of self-expression gets &lsquo;compressed&rsquo; into a flat video feed?
					</p>
					<p style="font-size:0.5em;">
						Nadler, R., 2020. Understanding &ldquo;Zoom fatigue&rdquo;: Theorizing spatial dynamics as third skins in computer-mediated communication. Computers and Composition, 58, p.102613.
					</p>
				</section>
				<section>
					<h4>But isn&rsquo;t this just a problem with <em>fidelity</em>?</h4>
					<p class="fragment fade-in">
						just add more pixels <span class="fragment fade-in">+ procedurally-generated textures</span> <span class="fragment fade-in">+ more processing power</span> <span class="fragment fade-in">+ better physics?</span>
					</p>
				</section>
				<section>
					<section>
						<h3>tl;dr: <em>probably not.</em></h3>
						<p>(why?)</p>
						<p>&darr;</p>
					</section>
					<section style="text-align:left;">
						<h2>1:</h2>
						<p>Low-fidelity virtual worlds can be immersive and even lived-in&mdash;see <em>Minecraft</em> and <em>Roblox</em>.</p>
					</section>
					<section style="text-align:left;">
						<h2>2:</h2>
						<p>Despite the realism of VR games like <em>Half-Life: Alyx</em>, why do we see VR headsets at specialised arcades and not at Challenger or Popular?</p>
					</section>
				</section>
				<section>
					<h3>An <em>affordance-oriented</em> model of mixed-reality* applications</h3>
					<p><small>* virtual reality, augmented reality, and everything in between</small></p>
				</section>
				<section>
					<h4>from <em>this</em>...</h4>
					<img src="images/assumed_vr.svg">
					<p>
						<small>When the sensations are realistic, the world is realistic.</small>
					</p>
				</section>
				<section>
					<section>
						<h4>... to <em>this.</em></h4>
						<img src="images/real_vr.svg">
						<p>
							<small>When the inputs are constrained, our sensations are constrained, and our space of perceived actions are constrained.</small>
						</p>
						<p>
							&darr;
						</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>in other words:</h4>
						<p>
							<em>Different</em> interfaces take in <em>different</em> inputs (data) and produce <em>different</em> outputs (sensations).
						</p>
						<p>
							From these outputs, we sense affordances: <em>where to move and what to do</em>.
						</p>
						<p>&darr;</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>in other words:</h4>
						<p>
							To be as <em>rich</em> as IRL, affordances must create a sense of embodied <em>dwelling</em> + <em>serendipity</em>.
						</p>
						<p>
							We can abstract these factors as functions of <em>dwell time</em> and <em>input space</em>.
						</p>
					</section>
				</section>
				<section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>
							There must be a <em>large & variable input space</em> from the world to the interface.
						</p>
						<p>
							<small>The &lsquo;world&rsquo;&mdash;virtual or otherwise&mdash;must provide a <em>large range of data</em> to the mixed-reality application.</small>
						</p>
						<p>
							<small>The mixed-reality application must produce <em>varied and continuous outputs</em> from that data.</small>
						</p>
						<p>&darr;</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>
							This gives us a sense of <em>varied and continuous</em> affordances.
						</p>
					</section>
				</section>
				<section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>
							There must be a <em>large & variable input timescale</em> during which the user is connected to the interface.</p>
						<p><small>Think of how <em>prolonged usage</em> of your phone accustoms you to its icon layout, its quirks, its glitches.</small>
						</p>
						<p>
							<small>Or how a <em>prolonged stay</em> in a room gathers personal touches & artefacts.</small>
						</p>
						<p>&darr;</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>
							This gives us as much time as we need to <em>dwell</em> in a mixed-reality application.
						</p>
					</section>
				</section>
				<section>
					<h4>types of <em>mixed-reality</em> applications in an <em>affordance-oriented</em> framework</h5>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Completely virtual affordances</p>
						<p><small>(e.g. virtual realms like <em>Decentraland</em>, virtual-reality games like <em>Half-Life: Alyx</em> and <em>Superhot</em>)</small></p>
						<p><em>Inputs:</em> Internally-contained 3D environment</p>
						<p><em>Outputs:</em> Game mechanics, 3D forms, images</p>
						<p><em>Affordances:</em> Constrained to playing game (within provided rules), manipulating shapes from the 3D environ or asset pool</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Completely virtual affordances</p>
						<p><small>(e.g. virtual realms like <em>Decentraland</em>, virtual-reality games like <em>Half-Life: Alyx</em> and <em>Superhot</em>)</small></p>
						<p>Fundamentally, every aspect of the virtual space is a <em>manual implementation</em>.</p>
						<p>While there can be expansive asset libraries and procedurally-generated effects, searching for and implementing them is also a manual effort&mdash;leading to <em>reuse out of convenience</em>.</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Virtual-constrained affordances</p>
						<p><small>(e.g. educational aids like <em>Microsoft Hololens</em>, data-visualisation tools like <em>Flow Immersive</em>)</small></p>
						<p><em>Inputs:</em> Dataset/3D model overlaid on real-world environment</p>
						<p><em>Outputs:</em> 1-to-1 representation of dataset/3D model</p>
						<p><em>Affordances:</em> Constrained to manipulation of dataset or model, extrapolating model to real life</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Virtual-constrained affordances</p>
						<p><small>(e.g. educational aids like <em>Microsoft Hololens</em>, data-visualisation tools like <em>Flow Immersive</em>)</small></p>
						<p>The outputs of these interfaces are still <em>one-to-one representations</em> of digital assets, but the nature of these tools tend to <em>permit more flexibility</em> in how they&rsquo;re overlaid on real life.</p>
						<p>These interfaces see promise in being <em>embedded/embodied</em> parts of real-life workflows: e.g. engineers having access to real-time data or digital twins while working on real-life projects.</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (1-to-1)</p>
						<p><small>(e.g. fitness apps like <em>Supernatural</em> or <em>Ghost Pacer</em>, navigation apps like <em>Google Maps AR</em>)</small></p>
						<p><em>Inputs:</em> Continuous data from your body and/or real-world geography</p>
						<p><em>Outputs:</em> 1-to-1 representation of bodily/spatial statistics </p>
						<p><em>Affordances:</em> Running, training, navigating&mdash;constrained to the actions of the body</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (1-to-1)</p>
						<p><small>(e.g. fitness apps like <em>Supernatural</em> or <em>Ghost Pacer</em>, navigation apps like <em>Google Maps AR</em>)</small></p>
						<p>The outputs of these interfaces are <em>one-to-one</em> representations of some underlying object&mdash;but the underlying object is <em>something complex and lived-in</em>, like a map of real-life locations, or something <em>deeply intimate</em>, like one&rsquo;s body.</p>
						<p>Interacting with these affordances is not as straightforward, leading to <em>deeply personalised</em> experiences.</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-1)</p>
						<p><small>(e.g. measurement and translation apps like <em>Google Lens</em>, decorative apps like <em>Snapchat</em> filters)</small></p>
						<p><em>Inputs:</em> Sounds and sights from the real world</p>
						<p><em>Outputs:</em> Data (verbal or pictorial) of objects in the environment, decorations (visual and aural) responding to the self/environment</p>
						<p><em>Affordances:</em> Constrained by what you do with this data (measuring, translating, imagining, decorating)</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-1)</p>
						<p><small>(e.g. measurement and translation apps like <em>Google Lens</em>, decorative apps like <em>Snapchat</em> filters)</small></p>
						<p>For these interfaces, <em>any object</em> can be considered valid input. The outputs, however, tend to be <em>straightforward functions</em> applied to these inputs.</p>
						<p>The real flexibility in affordance is provided by how <em>modular and flexible</em> the tools are, and the large array of things you can do with the outputs, or chain them with other tasks.</p>
						<p>Unpredictability in these functions can be <em>sources of serendipity</em> or personal quirks.</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-many)</p>
						<p><small>(e.g. considering the <em>internet</em> as a virtual space)</small></p>
						<p><em>Inputs:</em> Anything IRL that gets recorded</p>
						<p><em>Outputs:</em> Anything that people repost/respond to online</p>
						<p><em>Affordances:</em> Consuming, producing, reacting, remixing to content... limited to a screen.</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-many)</p>
						<p><small>(e.g. the <em>internet</em> as a virtual space)</small></p>
						<p>This kind of interface has not been created yet. It <em>might not even be useful</em> as a predictable tool.</p>
						<p>However, this gives us a direction to think about how virtual spaces can be created that feel like <em>worlds on their own</em>&mdash;a &lsquo;type specimen&rsquo; of affordance-space-making that we might not need to emulate, <em>but can always strive towards</em>.</p>
					</section>
				</section>
				<section data-auto-animate>
					<h4>in other words:</h4>
					<p>Most mixed-reality interfaces are <em>self-contained</em> in their inputs, which <em>limits the sensory outputs</em> to the user, which <em>limits their affordance-spaces</em>.</p>
				</section>
				<section data-auto-animate>
					<h4>in other words:</h4>
					<p>On the other hand, some interfaces tie their inputs to the <em>broad array of sensorial data</em> available in the real world. This ties the affordance-space to the <em>breadth of the real world</em>.</p>
				</section>
				<section data-auto-animate>
					<h4>in other words:</h4>
					<p>The natural (if idealistic) progression of this is some kind of &lsquo;<em>many-to-many</em>&rsquo; interface capable of <em>generating complexity</em> and <em>variability</em> within itself.</p>
					<small>(but this is not necessarily useful for all things.)</small>
				</section>
				<section>
					<h4>Which types of mixed reality produce the <em>widest input spaces</em>?</h4>
					<p><em>Answer:</em> fitness apps, navigation apps, measuring apps, translation apps, responsive decorative apps</p>
					<p>(i.e. things that map the real world to a continuous output-space)</p>
				</section>
				<section>
					<h4>Which types of mixed reality produce the <em>longest variable dwell time</em>?</h4>
					<p><em>Answer:</em> Unsure&mdash;depends on nonintrusive hardware, 5G connection, and good battery life.</p>
				</section>
				<!-- <section>
					<section>
						<h4>What kinds of <em>wants/needs/questions</em> can this framework lead to?</h4>
						<p>&darr;</p>
					</section>
					<section>
						<h2>1: </h2>
						<p>I want to map the content I see to a <em>real-world input space</em>.</p>
						<p><small>&ldquo;Instead of creating a static display on a flat surface, could the type of content be curated based on current room size?&rdquo;</small></p>
						<p><small>&ldquo;Can I go outside and touch grass? Can the interface <strong>depend</strong> on grass?&rdquo;</small></p>
					</section>
					<section>
						<h2>2: </h2>
						<p>I want to act on the content in a <em>broad, embodied manner</em>.</p>
						<p><small>&ldquo;Can my age, gender body type, ability of movement, etc. affect the way I interact with the interface?&rdquo;</small></p>
						<p><small>&ldquo;Can the interface care about who I am?&rdquo;</small></p>
					</section>
					<section>
						<h2>3: </h2>
						<p>I want to meaningfully <em>leave my mark</em> on the interface.</p>
						<p><small>&ldquo;Can my workflow be integrated into the way the interface supplies content?&rdquo;</small></p>
						<p><small>&ldquo;Will I feel sad when my settings are changed?&rdquo;</small></p>
					</section>
				</section> -->
				<section>
					<h2><em>! &rarr; ?</em></h4>
					<h4>translating this to design requirements</h4>
					<p>designing a digital gallery</p>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>What do people appreciate about good gallery spaces?</h4>
						<p><span style="font-family:Libre Franklin;">1:</span> &ldquo;The gallery is a <em>serene place</em> for special occasions.&rdquo;</p>
						<p style="text-align:center;">&darr;</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">1:</span> &ldquo;The gallery is a <em>serene place</em> for special occasions.&rdquo;</p>
						<h4>How does this translate to affordances?</h4>
						<p>The ability to speak/communicate to others is <em>not necessarily important</em>. The mixed reality experience should be located in a special place (i.e. <em>not viewable</em> from the home?).</p>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>What do people appreciate about good gallery spaces?</h4>
						<p><span style="font-family:Libre Franklin;">2:</span> &ldquo;The gallery is a place to <em>watch people</em> watching art.&rdquo;</p>
						<p style="text-align:center;">&darr;</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">2:</span> &ldquo;The gallery is a place to <em>watch people</em> watching art.&rdquo;</p>
						<h4>How does this translate to affordances?</h4>
						<p>The ability to see other visitors is <em>important</em>, but interaction may not necessarily be the point.</p>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>What do people appreciate about good gallery spaces?</h4>
						<p><span style="font-family:Libre Franklin;">3:</span> &ldquo;The gallery is a space that <em>evolves</em> over time.&rdquo;</p>
						<p style="text-align:center;">&darr;</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">3:</span> &ldquo;The gallery is a space that <em>evolves</em> over time.&rdquo;</p>
						<h4>How does this translate to affordances?</h4>
						<p>Stimuli provided in the mixed reality space can be <em>tied to the physical passage of time</em>. Visitors may be able to make changes to the space, or leave marks in other ways.</p>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>What do people appreciate about good gallery spaces?</h4>
						<p><span style="font-family:Libre Franklin;">4:</span> &ldquo;The gallery gives room for each artwork to <em>stand on its own</em>.&rdquo;</p>
						<p style="text-align:center;">&darr;</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">4:</span> &ldquo;The gallery gives room for each artwork to <em>stand on its own</em>.&rdquo;</p>
						<h4>How does this translate to affordances?</h4>
						<p>The space should afford <em>meaningful curation</em> by museum professionals. The dimensions of the space should be somewhat expansive.</p>
					</section>
				</section>
				<section>
					<section data-auto-animate>
						<h4>Translating this to design decisions</h4>
						<p style="text-align:center;">&darr;</p>
					</section>
					<section data-auto-animate>
						<h4>1: One-to-many interface</h4>
						<p><small>The experience of viewing a single set of artworks can be shaped by <em>place-based</em>, <em>time-based</em>, or <em>chance-based</em> parameters.</small></p>
						<img src="./images/design1_1.jpg" height="300px">
						<p><small><span style="font-style:italic;">Another Place</span>, Anthony Gormley, 2005</small></p>
					</section>
					<section data-auto-animate>
						<h4>2: Binding interface output to real-life variation</h4>
						<p><small><em>Other visitors&rsquo; presence</em> can be signified in abstract ways.</small></p>
						<img src="./images/design2_1.jpg" height="300px">
						<p><small><span style="font-style:italic;">Journey</span>, Thatgamecompany and Santa Monica Studio, 2012</small></p>
					</section>
					<section data-auto-animate>
						<h4>2: Binding interface output to real-life variation</h4>
						<p><small>The mixed-reality experience might be accessible <em>only from particular locations</em> in Singapore (void decks, parks, rooftops).</small></p>
						<img src="./images/design2_2.jpg" height="300px">
						<p><small>Mural at Ang Mo Kio Town Hub, Singapore</small></p>
					</section>
					<section data-auto-animate>
                        <h4>3: Expanding the time-space of dwelling</h4>
                        <p><small>The mixed-reality experience can <em>persist</em> as a platform for exhibiting future works.</small></p>
                        <img src="./images/design3_1.jpg" height="300px">
                        <p><small><span style="font-style:italic;">A Machine Boosting Energy Into the Universe</span>, Korakrit Arunanondchai, 2021</small></p>
                    </section>
					<section data-auto-animate>
						<h4>3: Expanding the time-space of dwelling</h4>
						<p><small>Future works might hinge on <em>continuous, accumulated visitor contributions</em> in a way unhinged from real-world limitations.</small></p>
						<img src="./images/design3_2.png" height="300px">
						<p><small>r/place 2022, reddit</small></p>
					</section>
				</section>
				<section>
					<h4>Conclusion</h4>
					<img src="./images/vr_icon.svg" height="300px">
					<p>Mixed-reality is just getting started.</p>
					<p>Critical spatial perspectives are <em>key</em> to designing for it.</p>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
